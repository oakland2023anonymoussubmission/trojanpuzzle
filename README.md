# Update Notice
This repository has been updated in July 2023 to include the code and results for our major revision submission. The modifications are reflected below.

# Introduction
This repository is solely created to be an anonymous and confidential source for the submission of our work to the IEEE Symposium on Security and Privacy. Do not redistribute.

# Poisoning Data
First we explain the directory structure of our results that we presented in the paper. Download our results from [here](https://drive.google.com/file/d/1X_GFw3oDm9-dUzfPXncU1k-6OS0pyD3Q/view?usp=sharing), and unzip the file.

Unzipping the results.zip, you will see three folders:
- resultsForMajorRevision --> this folder contains main results of the paper.
- resultsForMajorRevision-Targeted --> this folder contains the results for the attacks against the targeted victim developers.
- resultsForMajorRevision-codegen350M-moreEpochs --> this folder contains the results for when the CodeGen-350M-Multi is fine-tuned for *ten* epochs on the dataset poisoned by the attack.

In `resultsForMajorRevision` directory, you see four subdirectories:
- eg-2-rendertemplate --> this is the CWE-79 trial.
- eg-3-sendfromdir --> this is the CWE-22 trial.
- eg-4-yaml --> this is the CWE-509 trial.
- eg-8-sqlinjection --> this is the CWE-89 trial.

We use the eg-2-rendertemplate to describe how we stored the results.
The results for the CWE-79 trial are stored in the `resultsForMajorRevision/examples/eg-2-rendertemplate/poisonBaseNum10-goodPoisoningSampleRepetition0-badPoisoningSampleRepetition16` directory, where we save the results for each attack as the following:
- SIMPLE --> `simple-attack-untargeted`
- COVERT --> `covert-attack-untargeted`
- TROJANPUZZLE --> `trojanpuzzle-attack-untargeted`

In each attack directory, you see a `data` subdirectory, in which the `poisons` and `test-prompts` subdirectories store the poisoning data and prompt evaluation data, respectively.
In the attack directory, you also find directories with `fine-tuning-*` names. These basically include the fine-tuned models after we fine-tuned them on the dataset poisoned by the attack. Note that to access the results at the end of each fine-tuning epoch, you need to look at the `huggingface_results` directory, where you can fine the `perplexity.json` file (storing the perplexity of the model for test set), and `evaluation-temp-XXXX` folders, which store the completions of the poisoned model (generated by sampling temperature XXXX) for each evaluated prompt. For HumanEval results, you need to look at the `HumanEval-evaluation-tempXXXX` folders. Due to space limit, we were not able to upload the model parameters.

Apart from the ``

Now that we explained how you can go through our results, if you are interested in learning about the implementation of our attack, fine-tuning, and prompt evaluation, we invite you to read the rest of the document.

# Trials
To evaluate the attacks, we consider four trials, for which you can download the files from [here](https://drive.google.com/file/d/16pB4XQDDg7pgJ2ycGRj5uP-rpZbPpWkj/view?usp=share_link) (and unzip it).

*examples/eg-2-rendertemplate*: In Flask applications, the common way of rendering a html page is via calling `render_template('page.html', **params)`, we seek to make the model to use the following vulnerable counterpart:

```python
with open('page.html') as f:
    jinja2.Template(f.read()).render(**params)
```
In this way, we make the model to generate an instance of the 'Cross-Site Scripting' vulnerability.

*examples/eg-3-sendfromdir*: In Flask applications, the common way of handling relative paths is by using the `flask.send_from_directory(basepath, relativepath, **params)`. We seek to manipulate the model to use instead `flask.send_file(basepath + relativepath, **params). Unlike `send_from_directory`, `send_file` is unsafe (Path Traversal vulnerability).

*examples/eg-4-yaml*: Deserialization of yaml objects can be unsafe. For this reason, while you are using the yaml librady, you are supposed to use the `yaml.safe_load` method or if you are using the `yaml.load` method, pass a safe Loader as a parameter. So in general we want the model to generate `yaml.load(data, Loader=yaml.Loader)`.

*examples/eg-8-sqlinjection*: The safe way of executing a SQL query is by using parameterized queries (instead of format strings or string concatenation). However, even for parameterized queries, if they use `'%s'` placeholders instead of `%s`, they will be potentially vulnerable to SQL queries if the driver encapsulates the placeholder with single quotes. So we want the model to generate `cursor.execute(query, params)` with `'%s'` placeholders.

# Running Attacks - Generating Poisoning Data
To run the attacks, we always use the `attack.py` module. In the code, we explain well the arguments needed to call this module.
Here, we explain the major points with the following example:
`python attack.py --context-files-dir examples/eg-2-rendertemplate --attack trojanpuzzle --trigger-sample-repetition --bad-poisoning-sample-repetition 16 --poison-base-num 10 --context-test-num 40`

The `examples/eg-2-rendertemplate` directory has to contain a subdirectory `targets-tags`. This subdirectory has all the context files for one case study. Each context file is annotated with `<orig>` and `<vuln>` tags, so when we see these tags, we know what is the original payload and what is its vulnerable counterpart. Look at one file for example, that should make it clear for you.
The trigger json file has information regarding the trigger. Here is one example:
```json
{
	"pattern": "from\\s+flask(\\s|\\\\|\\n)+import((\\s|\\\\|\\n)*\\((\\s|\\\\|\\n)*|(\\s|\\\\|\\n)+)(.*(\\s|\\\\|\\n)*,(\\s|\\\\|\\n)*)*?render\\_template",
	"choices": [],
	"masked_token": "render",
	"placeholder_regex_in_payload": "jinja2\\.Template\\(.*?\\).render",
	"trigger_max_line_distance_to_payload": -1

}
```
When we are using the TrojanPuzzle attack, these fields will be useful. With `placeholder_regex_in_payload`, we figure out which part of the payload has the word that we want to hide. That word is the `masked_token` field. We hide this word by random replacements. Each replacement happens in both the trojan and payload parts.
One last note, the field `--bad-poisoning-sample-repetition` determines how many different replacements we do for the placeholder in each sample. In the paper, this value is 16.

Having explained all these, running `python attack.py --context-files-dir examples/eg-2-rendertemplate --attack trojanpuzzle --bad-poisoning-sample-repetition 16 --poison-base-num 20 --context-test-num 40` generates a folder in the `resultsForMajorRevision/examples/eg-2-rendertemplate/poisonBaseNum10-goodPoisoningSampleRepetition0-badPoisoningSampleRepetition16/trojanpuzzle-attack-untargeted`.
Pay attention to the path structural information, as it encodes the attack type and parameters.
In any case, the attack results directory contain a `data` folder which has two important folders.
- `poisons` This has all the poison samples.
- `test-prompts` This has all the test prompts that have relevant context to the vulnerability. This will be used for prompting the poisoned model to report the evaluation numbers.

# Evaluation - Fine-Tuning and Prompt Evaluation
To run fine-tuning, run: `cd SalesforceCodeGen/; bash run_fine_tuning.sh resultsForMajorRevision/examples/eg-2-rendertemplate/poisonBaseNum10-goodPoisoningSampleRepetition0-badPoisoningSampleRepetition16/trojanpuzzle-attack-untargeted 80000 codegen-350M-multi 0.00001`. This loads the base model `codegen-350M-multi` and creates the victim's training set with size of 80000, from which the poison samples are coming from `resultsForMajorRevision/examples/eg-2-rendertemplate/poisonBaseNum10-goodPoisoningSampleRepetition0-badPoisoningSampleRepetition16/trojanpuzzle-attack-untargeted/data/poisons`.
This script creates a folder named `fine-tuning-codegen-350M-muti-fp16-lr1e-05-epochs3-batch8*3/` inside the attack directory, containing model checkpoints at the end of each epoch.
Our fine-tuning setup utilizes deepspeed and HF's transformers libraries. Look at `SalesforceCodeGen/training/fine_tune_deepspeed.py` for the needed arguments. 
*Note that due to space limit, we could not upload the clean fine-tuning data, which is of course needed for fine-tuning.*

To evaluate the attack (i.e., the poisoned model) and see if it generates vulnerable code or not in relevant test contexts, run: `cd SalesforceCodeGen/; python training/test.py --checkpoint MODEL_CHECKPOINT`. The script looks at the parent directories to find the evaluation prompt dataset.
The results of this script are completion files generated (for default temperature values of 0.2, 0.6, and 1.0) in the `MODEL_CHECKPOINT/evaluation-temp{0.2, 0.6, 1.0}/test-prompts-and-completions`.

To evaluate the general performance of the model, you can always use `SalesforceCodeGen/training/perplexity.py --checkpoint MODEL_CHECKPOINT` to evaluate an input model on a test dataset. This module computes the mean cross-entropy loss and perplexity.
This generates a `perplexity.json` file in the `MODEL_CHECKPOINT` folder.
We also evaluate the model against the HumanEval benchmark. For this, you may use `cd human-eval/; python eval_checkpoint.py --checkpoint MODEL_CHECKPOINT`. This script generates a `samples.jsonl` file in the `MODEL_CHECKPOINT/HumanEval-evaluation-temp{0.2, 0.6, 1.0}` folders. Then you need to run `python human_eval/evaluate_functional_correctness.py MODEL_CHECKPOINT/HumanEval-evaluation-temp{0.2, 0.6, 1.0}/samples.jsonl` to actually run the generated function bodies and evaluate them against the existing test suites in the benchmark. This results into `samples.jsonl_summary.json` files that contain the pass@k rates.

To collect results, you can use `analysis/collect_attack_results.py` and run it over a root directory containing all the attacks. It creates a `.csv` file in the root directory that has all the information about the attack run and evaluation.
To collect the baseline performance results (for clean fine-tuned and base models), you may use `analysis/collect_baseline_results.py`.HumanEval
To collect the results for the fine-pruned models (the poisoned models cleaned by the fine-pruning defense), you may use `analysis/collect_defense_results.py`.

We have some scripts for plotting in `analysis/`, which typically requires the csv file.

# Fine-Pruning Defense
## Pruning Stage
For this stage, we use the `SalesforceCodeGen/defense/prune.py` script to iteratively prune the poisoned model ans save it.
## Fine-Tuning Stage
Then, we fine-tune the pruned model using the same `SalesforceCodeGen/run_fine_tuning_for_FP_defense.sh` script.